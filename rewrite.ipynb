{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Rewriting Pipeline "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the multi-hop rewriting pipeline that...\n",
    "- FIRST summarizes the abstract and title and \n",
    "- SECOND uses the summary to generate the self-contained rewrite. \n",
    "- THIRD makes the rewrite sound human-like"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"W05A4sw0uvDBLDle\"\n",
    "openai.api_base = \"https://praglab-dsp-proxy.omarkhattab.com:5533/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try: # When on google Colab, let's clone the notebook so we download the cache.\n",
    "    import google.colab\n",
    "    repo_path = 'dspy'\n",
    "    !git -C $repo_path pull origin v2 || git clone -b v2 https://github.com/stanfordnlp/dspy $repo_path\n",
    "except:\n",
    "    repo_path = '.'\n",
    "\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "# Set up the cache for this notebook\n",
    "os.environ[\"DSP_NOTEBOOK_CACHEDIR\"] = os.path.join(repo_path, 'cache')\n",
    "\n",
    "\"\"\"\n",
    "## TODO: Consider this\n",
    "if not \"dspy-ai\" in {pkg.key for pkg in pkg_resources.working_set}:\n",
    "    !pip install -U pip\n",
    "    !pip install -e $repo_path\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import dspy\n",
    "except Exception:\n",
    "    !pip install -U pip\n",
    "    !pip install -e $repo_path\n",
    "    \n",
    "    import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo', model_type='chat')\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "\n",
    "dspy.settings.configure(lm=turbo, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading QASPER json\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "with open('./getTemplate.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_in_multiple_lines\n",
    "def break_text_into_lines(text, max_words_per_line=35):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) <= max_words_per_line:\n",
    "            current_line.append(word)\n",
    "        else:\n",
    "            lines.append(' '.join(current_line))\n",
    "            current_line = [word]\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(' '.join(current_line))\n",
    "\n",
    "    return lines\n",
    "def print_in_multiple_lines(text, words_per_line):\n",
    "    lines = break_text_into_lines(text, words_per_line)\n",
    "    for line in lines:\n",
    "        print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Vanilla Rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_number = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRewrite(dspy.Signature):\n",
    "    \"\"\"Rewrite the question so that it is self-contained using the abstract and title. The rewrite should not include the title.\"\"\"\n",
    "\n",
    "    title = dspy.InputField()\n",
    "    abstract = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    rewrite = dspy.OutputField(desc=\"rewritten version of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Community Identity and User Engagement in a Multi-Community Landscape\n",
      "Abstract: A community's identity defines and shapes its internal dynamics. Our current understanding of this interplay is mostly limited to glimpses gathered from\n",
      "isolated studies of individual communities. In this work we provide a systematic exploration of the nature of this relation across a wide variety of online\n",
      "communities. To this end we introduce a quantitative, language-based typology reflecting two key aspects of a community's identity: how\n",
      "distinctive, and how temporally dynamic it is. By mapping almost 300 Reddit communities into the landscape induced by this typology, we reveal\n",
      "regularities in how patterns of user engagement vary with the characteristics of a community. Our results suggest that the way new and existing users engage with a\n",
      "community depends strongly and systematically on the nature of the collective identity it fosters, in ways that are highly\n",
      "consequential to community maintainers. For example, communities with distinctive and highly dynamic identities are more likely to retain their users. However, such niche\n",
      "communities also exhibit much larger acculturation gaps between existing users and newcomers, which potentially hinder the integration of the latter. More\n",
      "generally, our methodology reveals differences in how various social phenomena manifest across communities, and shows that structuring the\n",
      "multi-community landscape can lead to a better understanding of the systematic nature of this diversity.\n",
      "Question: How do the various social phenomena examined manifest in different types of communities?\n",
      "\n",
      "Predict:\n",
      "Rewrite 1: What are the manifestations of various social phenomena in different types of communities?\n",
      "Rewrite 2: How do various social phenomena manifest across different types of communities in the multi-community landscape?\n",
      "Rewrite 3: How do various social phenomena manifest across different types of communities in a multi-community landscape?\n",
      "Rewrite 4: How do different types of communities demonstrate the various social phenomena examined?\n",
      "Rewrite 5: What are the manifestations of various social phenomena in different types of communities?\n",
      "Chain of Thought:\n",
      "Rewrite 1: What regularities can be observed in the manifestation of various social phenomena across different types of communities in a multi-community landscape?\n",
      "Rewrite 2: What is the relationship between a community's identity and the manifestation of different social phenomena within it?\n",
      "Rewrite 3: What are the differences in the manifestation of social phenomena across different types of communities?\n",
      "Rewrite 4: How do different types of communities exhibit and experience various social phenomena?\n",
      "Rewrite 5: How does a community's identity influence patterns of user engagement in online communities?\n"
     ]
    }
   ],
   "source": [
    "# QASPER JSON\n",
    "generate_rewrite_predict = dspy.Predict(BasicRewrite, n=5)\n",
    "generate_rewrite_CoT = dspy.ChainOfThought(BasicRewrite, n=5)\n",
    "\n",
    "for idx, eachQuestion in enumerate(data):\n",
    "    if idx != question_number: continue\n",
    "    print(f'Title: {eachQuestion[\"title\"]}')\n",
    "    print_in_multiple_lines(f'Abstract: {eachQuestion[\"abstract\"]}', 30)\n",
    "    print(f'Question: {eachQuestion[\"question\"]}\\n')\n",
    "    \n",
    "    print('Predict:')\n",
    "    pred = generate_rewrite_predict(title=eachQuestion[\"title\"] , abstract=eachQuestion[\"abstract\"], question=eachQuestion[\"question\"])\n",
    "    for idx, c in enumerate(pred.completions): \n",
    "        print(f\"Rewrite {idx+1}: {c.rewrite}\")\n",
    "\n",
    "    print('Chain of Thought:')\n",
    "    pred = generate_rewrite_CoT(title=eachQuestion[\"title\"] , abstract=eachQuestion[\"abstract\"], question=eachQuestion[\"question\"])\n",
    "    for idx, c in enumerate(pred.completions): \n",
    "        print(f\"Rewrite {idx+1}: {c.rewrite}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Rewrite the question so that it is self-contained using the abstract and title. The rewrite should not include the title.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Title: ${title}\n",
      "\n",
      "Abstract: ${abstract}\n",
      "\n",
      "Question: ${question}\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the rewrite}. We ...\n",
      "\n",
      "Rewrite: rewritten version of the question\n",
      "\n",
      "---\n",
      "\n",
      "Title: PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry\n",
      "\n",
      "Abstract: Most approaches to emotion analysis regarding social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions that have been shown to also include mixed emotional responses. We consider emotions as they are elicited in the reader, rather than what is expressed in the text or intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the annotation of multiple labels per line to capture mixed emotions within context. We evaluate this novel setting in an annotation experiment both with carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of kappa=.70, resulting in a consistent dataset for future large scale analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52 F1-micro on the German subset. Data and resources are available at https://github.com/tnhaider/poetry-emotion\n",
      "\n",
      "Question: How is the annotation experiment evaluated?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the rewrite. We will first identify the key information in the abstract, which is the evaluation of the annotation experiment. Then, we will rephrase the question to make it self-contained.\n",
      "\n",
      "Rewrite: What is the evaluation of the annotation experiment conducted in the study?\u001b[0m\u001b[31m \t (and 4 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractRewrite(dspy.Signature):\n",
    "    \"\"\"Rewrite the question so that it is self-contained using the information given in the abstract only.\"\"\"\n",
    "\n",
    "    # title = dspy.InputField()\n",
    "    abstract = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    rewrite = dspy.OutputField(desc=\"rewritten version of the question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry\n",
      "Abstract: Most approaches to emotion analysis regarding social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or\n",
      "Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions that have been shown to also\n",
      "include mixed emotional responses. We consider emotions as they are elicited in the reader, rather than what is expressed in the text or\n",
      "intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the\n",
      "annotation of multiple labels per line to capture mixed emotions within context. We evaluate this novel setting in an annotation experiment both with\n",
      "carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of kappa=.70, resulting in a consistent dataset for future large scale\n",
      "analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52\n",
      "F1-micro on the German subset. Data and resources are available at\n",
      "https://github.com/tnhaider/poetry-emotion\n",
      "Question: How is the annotation experiment evaluated?\n",
      "\n",
      "Predict:\n",
      "Rewrite 1: How is the annotation experiment evaluated in terms of agreement and consistency?\n",
      "Rewrite 2: How is the annotation experiment evaluated and what is the result of the evaluation?\n",
      "Rewrite 3: How is the annotation experiment evaluated in terms of agreement and consistency?\n",
      "Rewrite 4: What is the result of the annotation experiment evaluation?\n",
      "Rewrite 5: How is the annotation experiment evaluated, and what is the resulting dataset's kappa value?\n",
      "Chain of Thought:\n",
      "Rewrite 1: How was the annotation experiment evaluated and with whom?\n",
      "Rewrite 2: How was the annotation experiment evaluated and what was the level of agreement with the experts?\n",
      "Rewrite 3: The annotation experiment is evaluated using carefully trained experts and crowdsourcing, leading to an acceptable agreement of kappa=.70 and resulting in a consistent dataset for future large scale analysis.\n",
      "Rewrite 4: What is the evaluation metric used to assess the annotation experiment?\n",
      "Rewrite 5: What methods were used to evaluate the annotation experiment, and what was the agreement level among the experts?\n"
     ]
    }
   ],
   "source": [
    "# Using abstract only => not really that good\n",
    "generate_rewrite_predict = dspy.Predict(AbstractRewrite, n=5)\n",
    "generate_rewrite_CoT = dspy.ChainOfThought(AbstractRewrite, n=5)\n",
    "\n",
    "for idx, eachQuestion in enumerate(data):\n",
    "    if idx != question_number: continue\n",
    "    print(f'Title: {eachQuestion[\"title\"]}')\n",
    "    print_in_multiple_lines(f'Abstract: {eachQuestion[\"abstract\"]}', 30)\n",
    "    print(f'Question: {eachQuestion[\"question\"]}\\n')\n",
    "    \n",
    "    print('Predict:')\n",
    "    pred = generate_rewrite_predict(title=eachQuestion[\"title\"] , abstract=eachQuestion[\"abstract\"], question=eachQuestion[\"question\"])\n",
    "    for idx, c in enumerate(pred.completions): \n",
    "        print(f\"Rewrite {idx+1}: {c.rewrite}\")\n",
    "\n",
    "    print('Chain of Thought:')\n",
    "    pred = generate_rewrite_CoT(title=eachQuestion[\"title\"] , abstract=eachQuestion[\"abstract\"], question=eachQuestion[\"question\"])\n",
    "    for idx, c in enumerate(pred.completions): \n",
    "        print(f\"Rewrite {idx+1}: {c.rewrite}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Summarize Abstract -> Rewrite Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarize(dspy.Signature):\n",
    "    \"\"\"Summarize the title and abstract into two sentence.\"\"\"\n",
    "\n",
    "    title = dspy.InputField()\n",
    "    abstract = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    summary = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryRewrite(dspy.Signature):\n",
    "    \"\"\"Rewrite the question so that it is self-contained using the summary.\"\"\"\n",
    "\n",
    "    summary = dspy.InputField()\n",
    "    question = dspy.InputField(desc=\"question to be rewritten\")\n",
    "    rewrite = dspy.OutputField(desc=\"rewritten version of the question\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compile SummaryRewrite using Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "Demo1 = dspy.Example(summary=\"The paper proposes a method for learning affective events using discourse relations, which is effective even without manually labeled data. The method only requires a small seed lexicon and a large raw corpus, and it improves supervised learning results when labeled data are limited.\", \n",
    "                     question=\"What is the seed lexicon?\", dspy_uuid=random.randint(0, 9999999), \n",
    "                     rewrite=\"What is the seed lexicon used in the paper on learning affective events using discourse relations, which is effective even without manually labeled data?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example({'summary': 'The paper proposes a method for learning affective events using discourse relations, which is effective even without manually labeled data. The method only requires a small seed lexicon and a large raw corpus, and it improves supervised learning results when labeled data are limited.', 'question': 'What is the seed lexicon?', 'rewrite': 'What is the seed lexicon used in the paper on learning affective events using discourse relations, which is effective even without manually labeled data?'}) (input_keys={'question', 'summary'})\n"
     ]
    }
   ],
   "source": [
    "examples = [Demo1]\n",
    "trainset = [x.with_inputs('summary', 'question') for x in examples]\n",
    "for x in trainset:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_rewrite = dspy.ChainOfThought(SummaryRewrite)\n",
    "    \n",
    "    def forward(self, summary, question):\n",
    "        prediction = self.generate_rewrite(summary=summary, question=question)\n",
    "        return dspy.Prediction(summary=summary, question=question, rewrite=prediction.rewrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot\n",
    "\n",
    "# Validation logic: check that the predicted answer is correct.\n",
    "# NO validation logic \n",
    "\n",
    "# Set up a basic teleprompter, which will compile our RAG program.\n",
    "teleprompter = BootstrapFewShot()\n",
    "\n",
    "# Compile!\n",
    "compiled_rag = teleprompter.compile(RAG(), trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry\n",
      "Abstract: Most approaches to emotion analysis regarding social media, literature, news, and other domains focus exclusively on basic emotion categories as defined by Ekman or\n",
      "Plutchik. However, art (such as literature) enables engagement in a broader range of more complex and subtle emotions that have been shown to also\n",
      "include mixed emotional responses. We consider emotions as they are elicited in the reader, rather than what is expressed in the text or\n",
      "intended by the author. Thus, we conceptualize a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the\n",
      "annotation of multiple labels per line to capture mixed emotions within context. We evaluate this novel setting in an annotation experiment both with\n",
      "carefully trained experts and via crowdsourcing. Our annotation with experts leads to an acceptable agreement of kappa=.70, resulting in a consistent dataset for future large scale\n",
      "analysis. Finally, we conduct first emotion classification experiments based on BERT, showing that identifying aesthetic emotions is challenging in our data, with up to .52\n",
      "F1-micro on the German subset. Data and resources are available at\n",
      "https://github.com/tnhaider/poetry-emotion\n",
      "Question: How is the annotation experiment evaluated?\n",
      "Summary: The study focuses on conceptualizing and annotating aesthetic emotions in poetry. The annotation experiment is evaluated by comparing the agreement between experts,\n",
      "resulting in an acceptable kappa value of .70.\n",
      "\n",
      "Zero-shot version:\n",
      "Rewrite 1: How is the annotation experiment in the study of conceptualizing and annotating aesthetic emotions in poetry evaluated?\n",
      "Compiled version:\n",
      "Rewrite 1: What method is used to evaluate the annotation experiment in the study on conceptualizing and annotating aesthetic emotions in poetry?\n"
     ]
    }
   ],
   "source": [
    "generate_summary = dspy.ChainOfThought(Summarize)\n",
    "generate_rewrite3 = dspy.ChainOfThought(SummaryRewrite, n=1)\n",
    "\n",
    "for idx, eachQuestion in enumerate(data):\n",
    "    if idx != question_number: continue\n",
    "    print(f'Title: {eachQuestion[\"title\"]}')\n",
    "    print_in_multiple_lines(f'Abstract: {eachQuestion[\"abstract\"]}', 30)\n",
    "    print(f'Question: {eachQuestion[\"question\"]}')\n",
    "\n",
    "    # Generate summary\n",
    "    pred = generate_summary(title=eachQuestion[\"title\"], abstract=eachQuestion[\"abstract\"], question=eachQuestion[\"question\"])\n",
    "    print_in_multiple_lines(f\"Summary: {pred.summary}\", 30)\n",
    "    summary = pred.summary\n",
    "\n",
    "    # Generate rewrite using the summary\n",
    "    print('\\nZero-shot version:')\n",
    "    pred = generate_rewrite3(summary=summary, question=eachQuestion[\"question\"])\n",
    "    for idx, c in enumerate(pred.completions): \n",
    "        print(f\"Rewrite {idx+1}: {c.rewrite}\")\n",
    "\n",
    "    print('Compiled version:')\n",
    "    pred = compiled_rag(summary=summary, question=eachQuestion[\"question\"])\n",
    "    print(f\"Rewrite {idx+1}: {pred.rewrite}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Rewrite the question so that it is self-contained using the summary.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Summary: ${summary}\n",
      "\n",
      "Question: question to be rewritten\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the rewrite}. We ...\n",
      "\n",
      "Rewrite: rewritten version of the question\n",
      "\n",
      "---\n",
      "\n",
      "Summary: The paper proposes a method for learning affective events using discourse relations, which is effective even without manually labeled data. The method only requires a small seed lexicon and a large raw corpus, and it improves supervised learning results when labeled data are limited.\n",
      "\n",
      "Question: What is the seed lexicon?\n",
      "\n",
      "Reasoning: Let's think step by step in order to produce the rewrite. We need to understand what the seed lexicon is in order to rewrite the question.\n",
      "\n",
      "Rewrite: What is the definition or purpose of the seed lexicon in the proposed method for learning affective events using discourse relations?\n",
      "\n",
      "---\n",
      "\n",
      "Summary: The study focuses on conceptualizing and annotating aesthetic emotions in poetry. The annotation experiment is evaluated by comparing the agreement between experts, resulting in an acceptable kappa value of .70.\n",
      "\n",
      "Question: How is the annotation experiment evaluated?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the rewrite. We need to understand how the annotation experiment is evaluated in order to rewrite the question.\n",
      "\n",
      "Rewrite: What method is used to evaluate the annotation experiment in the study on conceptualizing and annotating aesthetic emotions in poetry?\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Rewrite Human-like (yet to move)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Rubric (to select best rewrites from each pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 Examples (1~15)\n",
    "titles = [\"How Language-Neutral is Multilingual BERT?\", \n",
    "          \"How Language-Neutral is Multilingual BERT?\", \n",
    "          \"CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset\", \n",
    "          \"Learning Supervised Topic Models for Classification and Regression from Crowds\", \n",
    "          \"Learning Supervised Topic Models for Classification and Regression from Crowds\",\n",
    "          \"How Language-Neutral is Multilingual BERT?\",\n",
    "          \"Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\",\n",
    "          \"RobBERT: a Dutch RoBERTa-based Language Model\",\n",
    "          \"DENS: A Dataset for Multi-class Emotion Analysis\",\n",
    "          \"Unsupervised Machine Commenting with Neural Variational Topic Model\",\n",
    "          \"Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\",\n",
    "          \"Unsupervised Machine Commenting with Neural Variational Topic Model\",\n",
    "          \"Learning Supervised Topic Models for Classification and Regression from Crowds\",\n",
    "          \"DENS: A Dataset for Multi-class Emotion Analysis\",\n",
    "          \"Automatic Target Recovery for Hindi-English Code Mixed Puns\"]\n",
    "# abstracts = []\n",
    "questions = [\"How do they show that mBERT representations can be split into a language-specific component and a language-neutral component?\", \n",
    "             \"How do they show that mBERT representations can be split into a language-specific component and a language-neutral component?\", \n",
    "             \"What are the benchmark models?\", \n",
    "             \"What datasets were used?\", \n",
    "             \"What datasets were used?\",\n",
    "             \"What challenges does this work present that must be solved to build better language-neutral representations?\", \n",
    "             \"Which dataset do they use a starting point in generating fake reviews?\", \n",
    "             \"What data did they use?\",\n",
    "             \"What is the size of this dataset?\",\n",
    "             \"Which lexicon-based models did they compare with?\",\n",
    "             \"Which dataset do they use a starting point in generating fake reviews?\",\n",
    "             \"Which lexicon-based models did they compare with?\",\n",
    "             \"What datasets were used?\", \n",
    "             \"What is the size of this dataset?\",\n",
    "             \"What are Puns?\"]\n",
    "rewrites = [\"What method do the authors use to demonstrate that mBERT representations can be divided into a language-specific component and a language-neutral component in the paper \\\"How Language-Neutral is Multilingual BERT?\\\"?\", \n",
    "            \"What evidence do they provide to demonstrate that mBERT representations can be split into a language-specific component and a language-neutral component in the paper on the language-neutrality of mBERT?\",\n",
    "            \"What benchmark models are provided for pipelined task-oriented dialogue systems in the CrossWOZ dataset?\", \n",
    "            \"Which datasets were used in the study on learning supervised topic models for classification and regression from crowds?\",\n",
    "            \"What datasets were used in the paper on learning supervised topic models for classification and regression from crowds?\",\n",
    "            \"What challenges need to be addressed in order to improve the language-neutral representations in multilingual BERT?\", \n",
    "            \"What dataset do the authors use as a starting point in generating fake restaurant reviews in the paper?\",\n",
    "            \"What type of data did the authors use to train the Dutch language model RobBERT in the paper?\", \n",
    "            \"What is the average length of the passages in the DENS dataset for multi-class emotion analysis?\",\n",
    "            \"What lexicon-based models did the authors compare their proposed topic-based approach to in the paper on unsupervised machine commenting?\",\n",
    "            \"What dataset do the authors of the paper \\\"Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\\\" use as a starting point in generating fake reviews?\",\n",
    "            \"What are the lexicon-based models that the authors compare their proposed topic-based approach to in the paper?\",\n",
    "            \"What datasets were used to evaluate the proposed supervised topic models in the paper?\",\n",
    "            \"What is the size of the DENS dataset introduced in the paper?\",\n",
    "            \"What are puns and how are they classified in this paper?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Answers\n",
    "metric1 = [\"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\" , \"Yes\", \"Yes\", \"No\" , \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\"]   # Metric 1\n",
    "metric3 = [\"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\" , \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\" , \"No\" , \"Yes\", \"No\"]   # Metric 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 1: Same Answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SameAnswer(dspy.Signature):\n",
    "    \"\"\"Determine whether the rewrite is asking for the same thing as the original question at its core but with a specific focus on the study/paper.\"\"\"\n",
    "\n",
    "    title = dspy.InputField(desc=\"title of the paper\")\n",
    "    question = dspy.InputField(desc=\"the original question\")\n",
    "    rewrite = dspy.InputField(desc=\"the rewrite of the original question\")\n",
    "    answer = dspy.OutputField(desc=\"the answer in Yes or No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1\n",
      "Title: How Language-Neutral is Multilingual BERT?\n",
      "Question: How do they show that mBERT representations can be split into a language-specific component and a language-neutral component?\n",
      "Rewrite: What method do the authors use to demonstrate that mBERT representations can be divided into a language-specific component and a language-neutral component in the paper \"How Language-Neutral is Multilingual BERT?\"?\n",
      "Same Answer?: Yes\n",
      "Example 2\n",
      "Title: How Language-Neutral is Multilingual BERT?\n",
      "Question: How do they show that mBERT representations can be split into a language-specific component and a language-neutral component?\n",
      "Rewrite: What evidence do they provide to demonstrate that mBERT representations can be split into a language-specific component and a language-neutral component in the paper on the language-neutrality of mBERT?\n",
      "Same Answer?: Yes\n",
      "Example 3\n",
      "Title: CrossWOZ: A Large-Scale Chinese Cross-Domain Task-Oriented Dialogue Dataset\n",
      "Question: What are the benchmark models?\n",
      "Rewrite: What benchmark models are provided for pipelined task-oriented dialogue systems in the CrossWOZ dataset?\n",
      "Same Answer?: Yes\n",
      "Example 4\n",
      "Title: Learning Supervised Topic Models for Classification and Regression from Crowds\n",
      "Question: What datasets were used?\n",
      "Rewrite: Which datasets were used in the study on learning supervised topic models for classification and regression from crowds?\n",
      "Same Answer?: Yes\n",
      "Example 5\n",
      "Title: Learning Supervised Topic Models for Classification and Regression from Crowds\n",
      "Question: What datasets were used?\n",
      "Rewrite: What datasets were used in the paper on learning supervised topic models for classification and regression from crowds?\n",
      "Same Answer?: Yes\n",
      "Example 6\n",
      "Title: How Language-Neutral is Multilingual BERT?\n",
      "Question: What challenges does this work present that must be solved to build better language-neutral representations?\n",
      "Rewrite: What challenges need to be addressed in order to improve the language-neutral representations in multilingual BERT?\n",
      "Same Answer?: Yes\n",
      "Example 7\n",
      "Title: Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\n",
      "Question: Which dataset do they use a starting point in generating fake reviews?\n",
      "Rewrite: What dataset do the authors use as a starting point in generating fake restaurant reviews in the paper?\n",
      "Same Answer?: Yes\n",
      "Example 8\n",
      "Title: RobBERT: a Dutch RoBERTa-based Language Model\n",
      "Question: What data did they use?\n",
      "Rewrite: What type of data did the authors use to train the Dutch language model RobBERT in the paper?\n",
      "Same Answer?: Yes\n",
      "Example 9\n",
      "Title: DENS: A Dataset for Multi-class Emotion Analysis\n",
      "Question: What is the size of this dataset?\n",
      "Rewrite: What is the average length of the passages in the DENS dataset for multi-class emotion analysis?\n",
      "Same Answer?: No\n",
      "Example 10\n",
      "Title: Unsupervised Machine Commenting with Neural Variational Topic Model\n",
      "Question: Which lexicon-based models did they compare with?\n",
      "Rewrite: What lexicon-based models did the authors compare their proposed topic-based approach to in the paper on unsupervised machine commenting?\n",
      "Same Answer?: Yes\n",
      "Example 11\n",
      "Title: Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\n",
      "Question: Which dataset do they use a starting point in generating fake reviews?\n",
      "Rewrite: What dataset do the authors of the paper \"Stay On-Topic: Generating Context-specific Fake Restaurant Reviews\" use as a starting point in generating fake reviews?\n",
      "Same Answer?: Yes\n",
      "Example 12\n",
      "Title: Unsupervised Machine Commenting with Neural Variational Topic Model\n",
      "Question: Which lexicon-based models did they compare with?\n",
      "Rewrite: What are the lexicon-based models that the authors compare their proposed topic-based approach to in the paper?\n",
      "Same Answer?: Yes\n",
      "Example 13\n",
      "Title: Learning Supervised Topic Models for Classification and Regression from Crowds\n",
      "Question: What datasets were used?\n",
      "Rewrite: What datasets were used to evaluate the proposed supervised topic models in the paper?\n",
      "Same Answer?: Yes\n",
      "Example 14\n",
      "Title: DENS: A Dataset for Multi-class Emotion Analysis\n",
      "Question: What is the size of this dataset?\n",
      "Rewrite: What is the size of the DENS dataset introduced in the paper?\n",
      "Same Answer?: Yes\n",
      "Example 15\n",
      "Title: Automatic Target Recovery for Hindi-English Code Mixed Puns\n",
      "Question: What are Puns?\n",
      "Rewrite: What are puns and how are they classified in this paper?\n",
      "Same Answer?: Yes\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "generate_answer = dspy.ChainOfThought(SameAnswer)\n",
    "\n",
    "for idx in range(15):\n",
    "  # if idx != 10: continue\n",
    "  # Call the predictor on a particular input.\n",
    "  pred = generate_answer(title=titles[idx], question=questions[idx], rewrite=rewrites[idx])\n",
    "\n",
    "  # Print the input and the prediction.\n",
    "  print(f'Example {idx+1}')\n",
    "  print(f\"Title: {titles[idx]}\\nQuestion: {questions[idx]}\\nRewrite: {rewrites[idx]}\")\n",
    "  print(f\"Same Answer?: {pred.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Determine whether the rewrite is asking for the same thing as the original question at its core but with a specific focus on the study/paper.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Title: title of the paper\n",
      "\n",
      "Question: the original question\n",
      "\n",
      "Rewrite: the rewrite of the original question\n",
      "\n",
      "Reasoning: Let's think step by step in order to ${produce the answer}. We ...\n",
      "\n",
      "Answer: the answer in Yes or No\n",
      "\n",
      "---\n",
      "\n",
      "Title: Automatic Target Recovery for Hindi-English Code Mixed Puns\n",
      "\n",
      "Question: What are Puns?\n",
      "\n",
      "Rewrite: What are puns and how are they classified in this paper?\n",
      "\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the answer. We are asked to determine whether the rewrite is asking for the same thing as the original question at its core but with a specific focus on the study/paper. In the original question, the focus is on understanding what puns are. In the rewrite, the focus is on understanding what puns are and how they are classified specifically in the paper titled \"Automatic Target Recovery for Hindi-English Code Mixed Puns.\" Therefore, the rewrite is asking for the same thing as the original question but with a specific focus on the study/paper.\n",
      "\n",
      "Answer: Yes\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turbo.inspect_history(n=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 2: Includes Title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METRIC 5 Functions\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "from collections import Counter\n",
    "from dsp.utils.utils import print_message\n",
    "from dsp.utils.metrics import *    # import everything from dsp.utils.metrics\n",
    "\n",
    "def F1(prediction, answers_list):\n",
    "    assert type(answers_list) == list\n",
    "    return max(F1_score(prediction, ans) for ans in answers_list)\n",
    "def F1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_text(prediction).split()\n",
    "    ground_truth_tokens = normalize_text(ground_truth).split()\n",
    "\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if len(prediction_tokens) == len(ground_truth_tokens) == 0:\n",
    "        # Unlike most tasks, QReCC and SQuAD-2.0 assign 1.0 in this edge case. We don't for uniformity.\n",
    "        print_message(\n",
    "            \"\\n#> F1 Metric: Rare edge case of len(prediction_tokens) == len(ground_truth_tokens) == 0.\\n\")\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(prediction_tokens) # overlap in title\n",
    "    recall = 0 * num_same / len(ground_truth_tokens) # overlap in the rewrite (not important)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    f1 = precision\n",
    "\n",
    "    return f1\n",
    "def longest_common_substring(str1, str2):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "\n",
    "    # Create a 2D table to store the length of common substrings\n",
    "    # where dp[i][j] represents the length of common substring ending at str1[i-1] and str2[j-1]\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    # Variables to keep track of the length of the longest common substring and its ending index\n",
    "    max_len = 0\n",
    "    end_idx = 0\n",
    "    \n",
    "    # Fill in the table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "                if dp[i][j] > max_len:\n",
    "                    max_len = dp[i][j]\n",
    "                    end_idx = i\n",
    "\n",
    "    # Extract the longest common substring\n",
    "    longest_substring = str1[end_idx - max_len:end_idx]\n",
    "\n",
    "    return longest_substring\n",
    "def contiguous_ratio(title, rewrite):\n",
    "    title = normalize_text(title)\n",
    "    string = normalize_text(rewrite)\n",
    "    substring = longest_common_substring(title, string)\n",
    "    contig_ratio = len(substring)/len(title)\n",
    "    # print(\"Longest common substring:\", substring)\n",
    "    return contig_ratio\n",
    "def getCombinedScore(title, rewrite):\n",
    "    f1_score = F1(prediction=title, answers_list=[rewrite])\n",
    "    contiguous_score = contiguous_ratio(title, rewrite)\n",
    "    return f1_score * contiguous_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Metric5(example, rewrite):\n",
    "    combined_score = getCombinedScore(example.title, rewrite)\n",
    "    return \"No\" if (combined_score < 0.3) else \"Yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17350746268656717\n"
     ]
    }
   ],
   "source": [
    "# Metric 5 Tester\n",
    "title = \"Unsupervised Machine Commenting with Neural Variational Topic Model\"\n",
    "rewrite = \"What lexicon-based models did the authors compare their proposed topic-based approach to in the paper on unsupervised machine commenting?\"\n",
    "combined_score = getCombinedScore(title, rewrite)\n",
    "print(combined_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric 3: Specific Enough? (MOST IMPORTANT!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecificEnough1(dspy.Signature):\n",
    "    \"\"\"Answer the question\"\"\"\n",
    "    \n",
    "    rewrite = dspy.InputField()\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often 2 to 3 sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 12\n",
      "Title: Unsupervised Machine Commenting with Neural Variational Topic Model\n",
      "Rewrite: What are the lexicon-based models that the authors compare their proposed topic-based approach to in the paper?\n",
      "Specific Enough?: produce the answer. We need to identify the lexicon-based models that the authors compare their proposed topic-based approach to in the paper. To do this, we should carefully read the paper and look for any mentions or discussions about these models.\n",
      "\n",
      "Answer: In the paper, the authors compare their proposed topic-based approach to three lexicon-based models: SentiWordNet, V\n"
     ]
    }
   ],
   "source": [
    "# Define the predictor.\n",
    "Question1 = \"Given only the rewrite, how would one go about finding the correct paper using search?\"\n",
    "generate_answer = dspy.ChainOfThought(SpecificEnough1)\n",
    "\n",
    "for idx in range(15):\n",
    "  if idx != 11: continue\n",
    "\n",
    "  # Call the predictor on a particular input.\n",
    "  pred = generate_answer(rewrite=rewrites[idx])\n",
    "\n",
    "  # Print the input and the prediction.\n",
    "  print(f'Example {idx+1}')\n",
    "  print(f\"Title: {titles[idx]}\\nRewrite: {rewrites[idx]}\")\n",
    "  print(f\"Specific Enough?: {pred.answer}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Filter Best Rewrites using Rubric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring Method v1: \n",
    "- Same Answer => +2 points\n",
    "- DOESN'T Includes Title => +1 point\n",
    "- Specific Enough => +4 points\n",
    "Once we have this done, it will be used as the validation metric to train/optimize our rewrites AND used to filter the best rewrites..?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
